from bs4 import BeautifulSoup
import requests
import pandas as pd
import sqlite3
import json
import gmplot
import RAMESHSELVAM_KIRTHANA_hw5_plot
import sys
import operator
#the function that helps in scraping dataset 1 - malwaredomainlist.com, which contains the list of IP addresses of malware around the world and type of malware 
#this function will be responsible give the request to the each page in the site (With total 22 pages) and return the soup object to the other function that will scrape the required information
def scrapeDS1(pg):
    sess = requests.Session()
    adapter = requests.adapters.HTTPAdapter(max_retries=6)
    sess.mount('http://', adapter)
    url = "http://www.malwaredomainlist.com/mdl2.php?inactive=&sort=Date&search=&colsearch=All&ascordesc=DESC&quantity=100&page="+str(pg)
    soup = "N/A"
    try:
        req = sess.get(url)
        soup = BeautifulSoup(req.content, "lxml")
    except:
        print("retry fail")
    return soup
#function to store the dataset 1 into a SQL table using dataframes
def storeDS1(ipadd,tofmalware):
		malwaredf = pd.DataFrame({'IP_Address':ipadd,'Type_of_Malware':tofmalware})
		conn = sqlite3.connect('projectMal.db')
		malwaredf.to_sql('Malwaredata',conn,if_exists='replace')
		print("DS1 stored!")
#function to store dataset 2 into a SQL table using dataframes
def storeDS2(locdata):
		print("DS2 stored!")
		locdf = pd.DataFrame(locdata)
		conn = sqlite3.connect('projectMal.db')
		locdf.to_sql('Locationdata',conn,if_exists='replace')
locinfo = []
ip = []
#this function will scrape the required IP address and type of malware information from the respective <td> element from the web page (from the returned soup variable ) and append the IP addresses and type of malware to a list of dictionaries
def scrapeIPtype():
	typeofmalware=[]
	for i in range(1,23):
		s = scrapeDS1(i)
		td = s.findAll('td')
		for j in range(16,len(td),7):
			if str(td[j].text).find(':') != -1:
				ip.append(str(td[j].text[0:str(td[j].text).find(':')]))
				continue
			if str(td[j].text).find('/') != -1:
				ip.append(str(td[j].text[0:str(td[j].text).find('/')]))
			else:
				ip.append(str(td[j].text))
		for j in range(18,len(td),7):
			typeofmalware.append(td[j].text)
	storeDS1(ip,typeofmalware)
	return ip
#this function will help in accessing dataset 2, which is an API from where we need to get the location information and other information about the indicators of compromise
def scrapeDS2(ip):
	for i in ip:
		if str.isdigit(i[0]):
			scrapeDS2API(i)
	storeDS2(locinfo)
#this function will be used to compute the sum of all indicators of compromise that have been reported for a malware, which will be stored in the database
def findsumofioc(ioc):
	sum = 0
	for i,j in ioc.items():
		sum = sum+j
	return sum
#this function accesses the API and recieves respose in JSON format
#it is then appended to a list of dictionaries and sent to another function for storage
def scrapeDS2API(ipad):
	sess = requests.Session()
	adap = requests.adapters.HTTPAdapter(max_retries=6)
	sess.mount('https://',adap)
	info = dict()
	ds2 = 'https://otx.alienvault.com/api/v1/indicators/IPv4/'+str(ipad)+'/general'
	try:	
		req = sess.get(ds2)
		json_response = json.loads(req.text)
		info["ip"] = ipad 
		try:
			info["city"] = json_response["city"]
		except:
			info["city"] = "N/A"
		info["country"] = json_response["country_name"]
		info["latitude"] = json_response["latitude"]
		info["longitude"] = json_response["longitude"]
		try:
			inf = json_response["pulse_info"]
			s = findsumofioc(inf["pulses"][0]['indicator_type_counts'])
			info["ioc"] = s
		except:
			info["ioc"] = 1
	except:
		print("Still scraping.. Thank you for waiting!")
	locinfo.append(info)
				

	